---
title: "Molecular classification of cancer: a PCA approach"
author: "Xinyang Liu, Jinghan Cui"
date: "4/25/2022"
output:   
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,warning=FALSE,message = FALSE)
library(tidyverse)
library(ggplot2)
library(dplyr)
```

# Introduction
The project is inspired by the study published in 1999 by Golub _et al_ in which researchers developed a systematic approach to cancer classification based on global gene expression analysis using DNA microarrays. Traditionally, diagnosis of cancer has relied on histopathological appearance, but a serious limitation is that tumors with similar histopathological appearance may have different clinical courses and responses to therapy. 

Taking acute leukemias for example, there are two sub-types: acute lymphoblastic leukemia(ALL) and acute myeloid leukemia(AML). It is important to distinguish ALL from AML for target treatment.  The distinction between them can be well done in clinical practice, but misclassification may occur sometimes.

We developed a classification model based on the gene expression data. Out of thousands of genes, we tried to identify a small portion of gene with significantly different gene expression levels. If we have a new, unknown sample of acute leukemia, then researchers can perform gene sequencing on targeted gene we have identified and use the classification model as an assistance to the diagnosis of cancer sub-type.

# Data
We used the same data as Golub _et al_ used in their study. The dataset consists of quantitative expression levels of 7192 genes from 72 acute leukemia patients. The patients are labeled with Acute myeloid leukemia(AML) and Acute lymphoblastic leukemia(ALL) from previous clinical diagnoses. 

# Methods

Since this is a problem of "$p>>n$", dimension reduction should be done before we implement a logistic regression model. Hence, we will perform Principal Component Analysis (PCA) on the gene expression data for dimension reduction. Then we use the principal components to classify the types of leukemia. Comparing the accuracies of classification with different numbers of components, we pick the model with the best balance of sensitivity and specificity. Next, we calculate the bootstrap confidence interval for each component along with the z-score confidence interval and find the significant one. We then analyze the components using marginal correlation to identify genes that are differentially expressed between AML and ALL.

**Contribution**
All authors contributed equally to each part of the project including data analysis, writing, and presenting.

**Reference**

Golub, T. R., et al. “Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” Science, vol. 286, no. 5439, 1999, pp. 531–37, http://www.jstor.org/stable/2899325. Accessed 1 May 2022.

# Data analysis

First we did a data cleaning and since the gene expressions ranges differently, we did a standardization.

```{r}
#data cleaning 
rm(list = ls())
df1 <- read.csv("data_set_ALL_AML_train.csv")
df2 <- read.csv("data_set_ALL_AML_independent.csv")
label <- read.csv("actual.csv")
df <- cbind(df1,df2)
# extract patients columns
col_extract <- character(0)
for(i in 1:ncol(df)) {
  if(length(grep("X",colnames(df)[i]))>0) col_extract <- append(col_extract, colnames(df)[i])
}
df <- df[,col_extract]
rownames(df) <- df1[,2]
gene_name <- df1[,1]
df <- t(df)
n <- nrow(df)
p <- ncol(df)

# extract patient ID from row names and sort data by ID
ID <- numeric(n)
for(i in 1:n) ID[i] <- as.numeric(gsub("X","",rownames(df)[i]))
df <- data.frame(cbind(ID, df))
df <- df %>% arrange(ID)
X <- df[,-1]
Z <- label$cancer

scaled_X <- scale(X)
```

Next, we calculate the covariance matrix, eigenvalues and eigenvector.
```{r,eval = FALSE}
#the covariance matrix and eigen vector calculation take around 5-10 minutes to run.
#to save time for knitting the file we save the output for future use.
#this chunk will not be ran when knitting the file
#to re-calculate the two outputs, remove "eval=FALSE" and run this chunk
S <- cov(scaled_X)
eig <- eigen(S)

#save output
saveRDS(S,"cov_matrix.rds")
saveRDS(eig,"eigen.rds")
```

```{r}
S <- readRDS("cov_matrix.rds")
eig <- readRDS("eigen.rds")
cum_prop <- cumsum(eig$values/sum(eig$values))
cum_prop[1:100]
```

From the cumulative variance and scree plot, we found 70 components can explain 99% of the covariance. 
```{r}
# scree plot
plot(1:40, eig$values[1:40], type = "b", pch = 19,
     xlab = "i", ylab = expression(hat(lambda)), main = "Scree plot") 

plot(1:p, cum_prop, type = "l", pch = 19,
     xlab = "i", ylab = "cumulative proportion of variance", main = "") 
abline(a = 0.9, b = 0, lty = 20)

```

Due to the limitation of computing power, we decided to run logistic regression for 1 to 10 component. 10 component can explain approximate 50% of the variance.

The data is splited into train and test dataset at a ratio of 1:2. The accuracy of classification in the test dataset is calculated.

```{r, warning=FALSE,include=FALSE}
#accuracy
library(caret)
set.seed(1234)
#split 2/3 data to train and 1/3 to test
trainIndex <- createDataPartition(Z, p=0.67,
                                  list = FALSE,
                                  times =1)
Z_binary <- ifelse(Z=="ALL",1,0)
#run logistic regression from 1 to 10 components
k_seq <- 1:10
all_fit <- list()
tbl_accuracy <- data.frame(k = rep(NA_real_,length(k_seq)),
                           accuracy = NA_real_)
                       
for(i in 1:length(k_seq)) {
  k <- k_seq[i]
  print(k)
  tbl_accuracy$k[i] <- k
  Xk <- as.matrix(scaled_X) %*% eig$vectors[, 1:k] #principal components
  data <- as.data.frame(cbind(Xk,Z_binary))
  Train <- data[trainIndex,]
  Test <- data[-trainIndex,]
  fit <- glm(as.factor(Z_binary) ~ ., data=Train, family = "binomial") #logistic regression
  #all_fit[[i]] <- fit
  #calculate acccuracy of classification
  Test$model_prob <- predict(fit, newdata=Test, type = "response")
  Test <- Test  %>% mutate(model_pred = 1*(model_prob > .5) + 0)
  Test <- Test %>% mutate(accurate = 1*(model_pred == Z_binary))
  tbl_accuracy$accuracy[i] <- sum(Test$accurate)/nrow(Test) 
}
```
```{r }
#plot accuracy with number of components
ggplot(data=tbl_accuracy, aes(x=k, y=accuracy)) +
  geom_point() +
  geom_line() +
  labs(x="# of principal components")
```

From the accuracy plot we can see the accuracy reaches 100% when we include 5 principal components to the model and then decreases as we include more components but increases again to 95% when we include 10 components. We think this unstable results may be caused by the small sample size because when we include more than 5 components to the model, the model can not converge.

We chosed the model with three principal components as the 3-component model can converge and has an accuracy of 86%. This model should have a good balance between bias and variance. We want to run a bootstrap for this model and check whether the regression 95% confidence interval and the bootstrap 95% confidence interval agrees.

```{r,warning=FALSE}
#boostrap confidence interval
Xk_boot <- as.matrix(scaled_X) %*% eig$vectors[, 1:3] #principal components
data_boot <- as.data.frame(cbind(Xk_boot,Z_binary))
# Containers for the coefficients
sample_coef_intercept <- NULL
sample_coef_x1 <- NULL
sample_coef_x2 <- NULL

for (i in 1:1000) {
  #Creating a resampled dataset from the sample data
  sample_d = data_boot[sample(1:nrow(data_boot), nrow(data_boot), replace = TRUE), ]
  
  #Running the regression on these data
  model_bootstrap <- glm(as.factor(Z_binary) ~ ., data=sample_d, family = "binomial") #logistic regression
  
  #Saving the coefficients
  sample_coef_intercept <-
    c(sample_coef_intercept, model_bootstrap$coefficients[1])
  
  sample_coef_x1 <-
    c(sample_coef_x1, model_bootstrap$coefficients[2])
  
  sample_coef_x2 <-
    c(sample_coef_x2, model_bootstrap$coefficients[3])
  
  sample_coef_x3 <-
    c(sample_coef_x2, model_bootstrap$coefficients[4])
}

a <-
  cbind(
    quantile(sample_coef_intercept, prob = 0.025),
    quantile(sample_coef_intercept, prob = 0.975))
b <-
  cbind(quantile(sample_coef_x1, prob = 0.025),
        quantile(sample_coef_x1, prob = 0.975))
c <-
  cbind(quantile(sample_coef_x2, prob = 0.025),
        quantile(sample_coef_x2, prob = 0.975))
d <-
  cbind(quantile(sample_coef_x3, prob = 0.025),
        quantile(sample_coef_x3, prob = 0.975))

Xk <- as.matrix(scaled_X) %*% eig$vectors[, 1:3] #principal components
data <- as.data.frame(cbind(Xk,Z_binary))
Train <- data[trainIndex,]
Test <- data[-trainIndex,]
fit <- glm(as.factor(Z_binary) ~ ., data=Train, family = "binomial") #logistic regression
d <-
  round(cbind(
    sample = confint(fit),
    boot = rbind(a,b,c,d)), 4)
colnames(d) <- c("2.5 %", "97.5 %",
                 "2.5 %", "97.5 %")
knitr::kable(rbind(
  c('regression',
    'regression',
    'bootstrap',
    'bootstrap'),d))
```

Both the regression and boostrap confidence intervals agree that the first component is not significant and the second component is significantly negatively correlated with the outcome. While for the third component, the regression confidence interval shows it is positively correlated but the bootsrap confidence interval show it is negatively correlated. So we will further explore the second component.

We want to look into it's marginal correlations to determine which gene expression contributes most to this component by calculating the marginal correlations.

```{r}
# Marginal correlations:
marg_cor <- eig$vectors[,2]*sqrt(eig$values[2])
marg_cor <-  data.frame(marg_cor)
marg_cor <- marg_cor %>%
  mutate(abs_cor = abs(marg_cor), name = gene_name) 
```

We picked the 50 genes that contributes the most to the second component and draw a heatmap to see how thoes genes' expression levels are related to the outcome. ]

The left side of the heatmap are patients who are diagnosed with Acute myeloid leukemia(AML), and the right side are patients who are diagnosed with Acute lymphoblastic leukemia(ALL). We can see that most of the genes are much more down-regulated for ALL patients than for AML patients.

```{r}
library(RColorBrewer)
gene_id <- order(marg_cor$abs_cor, decreasing = T)[1:50]
patient_id <- order(label$cancer)
X1 <- scaled_X[patient_id, gene_id]
colnames(X1) <- gene_name[gene_id]
heatmap(t(X1),
        Rowv=NA, Colv=NA, col=rev(brewer.pal(9,"RdBu")))
```






